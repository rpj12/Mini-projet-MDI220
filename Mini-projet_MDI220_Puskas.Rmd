---
title: "Mini-projet MDI 220 Ruila Puskas-Juhasz"
output:
  html_notebook: default
  pdf_document: default
---

```{r}
data("discoveries")
plot(discoveries)
```
## Exercice 1
**1.** On cherche un estimateur $\hat{\theta}$ du jeu de donnees decrit par une loi geometrique. 
La densite de probabilite est donnee par $p(x, \theta) = \prod \theta (1-\theta)^{x_i}$ 
En prenant la log-vraisemblance on obtient $log(p(x,\theta)) = nlog(\theta) + \sum_{i=1}^{n} x_ilog(1-\theta)$
$\frac{\partial log(p(x,\theta))}{\partial \theta} = \frac{n}{\theta} - \frac{1}{1-\theta} \sum_{i=1}^{n} x_i = 0$
En annulant la derivee de cette expression par rapport à $\theta$, on obtient $\hat{\theta} = \frac{1}{1+\sum_{i=1}^{n} x_i}$

**2.** Dans le cas d'une loi de Poisson ou le parametre inconnu est $\lambda$, on trouve l'expression de $\hat{\lambda}$  en passant par la log-vraisemblance : 
$log(p(x, \lambda )) =  \sum_{} x_i log( \lambda ) -n\lambda - \sum_{} log(x_i !)$
En annulant la dérivée par rapport à $\lambda$ on obtient : 
$\frac{1}{\lambda} \sum{} x_i - n = 0$ 
$\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} x_i$
```{r}
#calcul de la moyenne et de la variance empirique du jeu de donnees
m = mean (discoveries)
v = var (discoveries)
print("moyenne empirique")
print(m)

print("variance empirique")
print(v)

#calcul à partir de l'estimateur de la loi géomtrique
n = length(discoveries)
S = 0 
for (i in (1:n)) {
  S <- S + discoveries[i]
}

theta = 1/((1/n)*S +1)
moy_geom = (1/n)*S + 1 ##1/theta
var_geom = (1-theta)/(theta**2)

print ('moyenne geometrique')
print(moy_geom)

print('variance geometrique')
print(var_geom)

#calcul à partir de l'estimateur de la loi de Poisson
lambda = (1/n)*S
moy_poisson = lambda
var_poisson = lambda
print('moyenne et variance de Poisson = ')  
print(lambda)
```
**3.**D'apres les resultats ci-dessus, la loi geometrique donne des resultats moins satisfaisants que la loi de Poisson. Le modele de Poisson est donc plus adapte. 
```{r}
print(m/moy_geom)
print(v/var_geom)

print(m/moy_poisson)
print(v/var_poisson)

print("les valeurs obtenues par la loi de Poisson sont plus proches des valeurs empiriques. La loi de Poisson approche le mieux la statistique consideree.")
```


**4.** D'apres les resultats ci-dessous, on observe que la courbe de la densite de Poisson se rapproche le plus de l'histogramme des valeurs empiriques puisqu'elle en reprend la même forme mais avec un décalage vers la droite. La premiere impression est donc confirmée 
Bleu : densite geometrique
Rouge : densite de Poisson
```{r}
x = 0:12
hist(discoveries, probability = TRUE, breaks = 12)
lines(x, dgeom(x, prob = theta, log=FALSE), main = 'densite geometrique', col='blue')
lines(x, dpois(x, lambda, log = FALSE), main = 'densite de Poisson', col='red')
```


**5.** La courbe de Henry de la distribution de Poisson convient le mieux. 

```{r}
##Utilisation de la fonction qqplot
#preparation des données

a<-rgeom(500, theta)
b<-rpois(500, lambda)


qqplot( a,discoveries, main= 'qqplot loi géométrique et discoveries', col='blue', ylim = range(0:13))
lines(0:13,0:13)

qqplot(b, discoveries, main= 'qqplot loi de Poisson et discoveries', col='red', ylim = range(0:13))
lines(0:13,0:13)
```
**6.** Comparaison avec la loi du Chi^2. Cette loi a cinq degrés de libertés puisque le partitionnement est de dimension 5.
Le modele qui convient le mieux est le modele de Poisson. 
```{r}
#c<-rchisq(2000, df = 5, ncp=0.05)
c<-qchisq(ppoints(2000), df = 5)

#rappel
a<-rgeom(2000, theta)
b<-rpois(2000, lambda)

## QQPlot pour la loi du Chi^2 comparée à la ditribution géométrique:
qqplot(c, a, main = expression("Q-Q plot loi geometrique et loi"~~ {chi^2}[nu == 5]), col='red')
lines(0:50, 0:50, col='black')

## QQPlot pour la loi du Chi^2 comparée à la ditribution de Poisson:
qqplot(c,b,main = expression("Q-Q plot loi de Poisson et loi"~~ {chi^2}[nu == 5]), col='blue')
lines(0:25, 0:25, col='black')

```
On observe sur les courbes ci-dessus que la loi qui convient le mieux est la loi de Poisson. 
On cherche maintenant la valeur de la somme. 
**Loi geometrique**

```{r}
liste_proba_geom <- list(0.0,0.0,0.0,0.0,0.0,0.0)
for (i in 1:5) {
  liste_proba_geom[i] <- dgeom(i-1, theta)
}

somme_liste <- liste_proba_geom[[1]] + liste_proba_geom[[2]] + liste_proba_geom[[3]] + liste_proba_geom[[4]] + liste_proba_geom[[5]]
liste_proba_geom[6] <- 1.0 - somme_liste

n_j <- list(0.0,0.0,0.0,0.0,0.0,0.0)
for (i in 1:5) {
  n_j[i] <- length(subset(discoveries, discoveries==i-1))
}
n_j[6] <- length(subset(discoveries, discoveries >=5))
print(n_j)


##Calcul de la somme
S <- 0
for (i in 1:6) {
  S <- S + ((n_j[[i]]- 100*liste_proba_geom[[i]])^2)/(100*liste_proba_geom[[i]])
}
print(S)

##Comparaison avec le quantile d'ordre alpha de la loi du Chi 2
q <- qchisq(0.95, df = 5)
q
if (S>q) {
  print('la loi geometrique ne convient pas')
}
```

**Loi de Poisson**

```{r}
#rappel
n_j <- list(0.0,0.0,0.0,0.0,0.0,0.0)
for (i in 1:5) {
  n_j[i] <- length(subset(discoveries, discoveries==i-1))
}
n_j[6] <- length(subset(discoveries, discoveries >=5))
print(n_j)



liste_proba_pois <- list(0.0,0.0,0.0,0.0,0.0,0.0)
for (i in 1:5) {
  liste_proba_pois[i] <- dpois(i-1, lambda)
}

somme_liste_tilde <- liste_proba_pois[[1]] + liste_proba_pois[[2]] + liste_proba_pois[[3]] + liste_proba_pois[[4]] + liste_proba_pois[[5]]
liste_proba_pois[6] <- 1.0 - somme_liste_tilde


##Calcul de la somme
S_tilde <- 0
for (i in 1:6) {
  S_tilde <- S_tilde + ((n_j[[i]]- 100*liste_proba_pois[[i]])^2)/(100*liste_proba_pois[[i]])
}
print(S_tilde)

##Comparaison avec le quantile d'ordre alpha de la loi du Chi 2
q <- qchisq(0.95, df = 5)
q
if (S_tilde<q) {
  print('la loi de Poisson convient')
}
```
**Donc la loi de poisson est acceptee**



## Exercice 2 : Analyse de l'incertitude dans le modele de Poisson
**1.** Les $(X_i)$ sont independantes et identiquement distribuees, suivant une loi de Poisson de parametre $\lambda$. Par addivite de la loi de Poisson, la variable aleatoire $(\sum{X_i})$ suit une loi de Poisson de parametre $n\lambda$. 
On pose $Y=\sum_{i=1}^{n}X_i$
Alors : $\forall s\ge1,  P(Y\ge s) = \sum_{k=s}^{+infty} P(Y=k) = \sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} \frac{(n \lambda)^k }{k!}$
En derivant terme a terme, on obtient : $\frac{\partial P(Y\ge s)}{\partial \lambda}=\sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} (-\frac{n^{k+1} \lambda^k}{k!} + \frac{n^k \lambda^{k-1}}{(k-1)!} )$
Il s'agit d'une serie entiere ayant pour terme general la difference de deux termes consecutifs. Ainsi, il vient :
$\frac{\partial P(Y\ge s)}{\partial \lambda}=\mathrm{e}^{-n\lambda} \frac{n^s \lambda^{s-1}}{(s-1)!} \ge 0$
Donc la fonction est croissante. 

**2.** On en déduit un test de de l'hypothèse nulle. 
On utilise le rapport de vraisemblance monotone, pour cela verifions que la fonction de vraisemblance est strictement croisssante $Z(x) = \frac{p_{\theta_2}(x)}{p_{\theta_1}(x)} = \prod_{k=1}^{n} (\frac{\theta_2}{\theta_1})^{x_i} \exp(-n(\theta_2 - \theta_1))$ 
$\Leftrightarrow Z(x) = \exp(-n(\theta_2 - \theta_1) + log(\frac{\theta_2}{\theta_1}) \sum_{k=1}^{n} x_i) = \exp(-n(\theta_2 - \theta_1) + log(\frac{\theta_2}{\theta_1}) T)$
Or on a pris $\theta_1 < \theta_2$ donc $log(\frac{\theta_2}{\theta_1}) > 0$. 
Ainsi : la fonction $T \longmapsto \frac{p_{\theta_2}}{p_{\theta_1}}$ est strictement croissante. 

T suit une loi de Poisson de parametre $n\lambda$. Sous l'hypothese nulle : $\lambda = 3$. 

On a le test suivant : $H_0 : \lambda \le 3$ et $H_1 : \lambda > 3$ avec $\alpha \le 0.05$.
On rappelle que le modele est $\theta_0 = \lbrack 0,3 \rbrack$ et $\theta_1 = \rbrack 3, + \infty\lbrack$. Il s'agit de determiner un zone de rejet telle que, sous l'hypothese nulle, la probabilité que X appartienne à cette zone $X_1$ soit inférieure ou egale a $5/100$. On s'interesse à la statistique $\delta (T) = \mathbb{1} {T>s}$. 

On exprime le risque de premiere espece : $R(\theta_0 , \delta) = \alpha = P(\delta(T) = 1) \le 0.05$
$\Leftrightarrow \alpha = P(T > s) \Leftrightarrow \alpha = 1-P(T\le s) \Leftrightarrow P(T\le s) = 1- \alpha$
On retrouve l'expression de la fonction de répartition de T, dont la reciproque s'exprime comme le quantile $q_1$ associe a la loi $P(3n)$. D'ou : $s = q_1(1-\alpha)$. 

 
```{r}
alpha = 0.05
s<- qpois(1-alpha, n*3)
print("s = ")
qpois(1-alpha, n*3)
```
**3.** On sait que la somme des découvertes de discoveries est de 310 et puisque $310 < 329$, on accepte l'hypothese nulle. 

**4.** On considere cette fois la moyenne empirique m de discoveries. Soit $n_0$ le nombre minimum de donnees necessaires pour pouvoir rejeter $H_0$. 
Pour rejeter le test, il faut : $T(X) > s$, donc : $\sum_{i=1}^{n}X_i > s \Leftrightarrow nm > s$.
Finalement : $n_0 = \lfloor \frac{s}{m} \rfloor +1$.
```{r}
# rappels : m=mean(discoveries)=3.1
print(floor(329/3.1)+1)

```


Application numérique : il faut au moins $n_0=107$ donnees pour rejeter $H_0$. 

**5.** La fonction puissance est donnee par : $\beta(\theta, \delta) = 1- R(\theta, \delta)$ pour $\theta \in \theta_1$. R est ici le risque de premiere espece (rejeter a tort $H_0$). On trace $\beta$ en fonction de $\lambda$. 

```{r}
######PLOT DE LA FONCTION PUISSANCE########
j <- 1
n<-100
a <- seq(3,4,0.01)
b <- seq(3,4,0.01)
s<-qpois(0.95, 300)

for (k in a) {
  b[j] <- 1 - ppois(s, n*k)
  j = j+1
}

plot(a,b, type='l', main = 'Trace de la fonction puissance en fonction de lambda')

```

Comme le montre la cellule ci-dessous, il faut 112 donnees pour que $0.9\le \beta(\lambda = 3.5)$
```{r}
n_donnees <- 1
beta_cible <- 1 - ppois(s, n_donnees*3.5)

while (beta_cible<0.9) {
  n_donnees = n_donnees +1
  
  beta_cible <- 1 - ppois(qpois(1-0.05, 3*n_donnees), n_donnees*3.5)
}

print(n_donnees)
```


## Exercice 3 (Analyse bayesienne)
**1.** La distribution du prior conjugue d'une loi de Poisson est une distribution Gamma. 
Donc $\lambda \hookrightarrow \Gamma(\theta, k)$. Alors $E(\lambda) = k\theta$ et $Var(\lambda) = k\theta^2$. 
Or d'apres les donnees, il vient : $k=0.25$ et $\theta = 20$. 

**2.** On calcule les expressions des parametres k et theta de la loi a posteriori $\pi$ en fonction de $x_{1:n}$ :
$\pi(\lambda / x) = p_\lambda(x)\pi(\lambda) = \pi(\lambda) \prod_{i = 1}^{n} p_\lambda(x_i)$ puisque les $x_i$ sont indépendantes. 
Donc $\pi(\lambda / x) \propto (\lambda ^{k-1} e^{-\frac{\lambda}{\theta}}) \prod_{i = 1}^{n} \frac{ \lambda^{x_i}e^{-\lambda}}{x_i !}$
Ainsi : $\pi(\lambda / x) \propto \lambda^{\sum x_i} e^{-n\lambda} \lambda ^{k-1} e^{-\frac{\lambda}{\theta}} \Rightarrow \pi(\lambda / x) \propto \lambda^{\sum x_i+k-1} e^{-n\lambda-\frac{\lambda}{\theta}}$
On identifie donc avec les parametres d'une loi Gamma : $(\sum_{i=1}^{n} x_i+k ; \frac{\theta}{n\theta+1}) = (k';\theta')$
L'estimateur de l'esperance a posterio s'exprime comme : $ab = (\sum_{i=1}^{n} x_i+k)\frac{\theta}{n\theta+1}$
L'estimateur du maximum de vraisemblance s'exprime comme : $\hat\beta = \frac{\sum_{i=1}^{n} x_i}{nk'} = \frac{\sum_{i=1}^{n} x_i}{n\sum_{i=1}^{n} x_i+k}$ pour estimer $\theta'$. Cependant, on ne connait pas $k$, cet estimateur ne nous permet donc pas d'estimer l'ensemble des parametres $k'$ et $\theta'$. 
EVM E(theta / x) = S/n en considérant 
**3.** On obtient : 
```{r}
k_a_estimer <- sum(discoveries)+ 0.25
theta_a_estimer <- 20/(n*20+1)
print(theta_a_estimer)
print(k_a_estimer)
```


