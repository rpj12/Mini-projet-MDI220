---
title: "Mini-projet MDI 220 Ruila Puskas-Juhasz"
output:
  html_notebook: default
  pdf_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
```{r}
data("discoveries")
plot(discoveries)
```
## Exercice 1
**1.** On cherche un estimateur $\hat{\theta}$ du jeu de donnees decrit par une loi geometrique. 
La densite de probabilite est donnee par $p(x, \theta) = \prod \theta (1-\theta)^{x_i}$ 
En prenant la log-vraisemblance on obtient $log(p(x,\theta)) = nlog(\theta) + \sum_{i=1}^{n} x_ilog(1-\theta)$
$\frac{\partial log(p(x,\theta))}{\partial \theta} = \frac{n}{\theta} - \frac{1}{1-\theta} \sum_{i=1}^{n} x_i = 0$
En annulant la derivee de cette expression par rapport à $\theta$, on obtient $\hat{\theta} = \frac{1}{1+\sum_{i=1}^{n} x_i}$

**2.** Dans le cas d'une loi de Poisson ou le parametre inconnu est $\lambda$, on trouve l'expression de $\hat{\lambda}$  en passant par la log-vraisemblance : 
$log(p(x, \lambda )) =  \sum_{} x_i log( \lambda ) -n\lambda - \sum_{} log(x_i !)$
En annulant la dérivée par rapport à $\lambda$ on obtient : 
$\frac{1}{\lambda} \sum{} x_i - n = 0$ 
$\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} x_i$
```{r}
#calcul de la moyenne et de la variance empirique du jeu de donnees
m = mean (discoveries)
v = var (discoveries)
print("moyenne empirique=")
print(m)
print("variance empirique=")
print(v)

#calcul à partir de l'estimateur de la loi géomtrique
n = length(discoveries)
S = 0 
for (i in (1:n)) {
  S <- S + discoveries[i]
}

theta = 1/((1/n)*S +1)
moy_geom = (1/n)*S + 1 ##1/theta
var_geom = (1-theta)/(theta**2)
print ('moyenne geometrique=')
print(moy_geom)
print('variance geometrique=')
print(var_geom)

#calcul à partir de l'estimateur de la loi de Poisson
lambda = (1/n)*S
moy_poisson = lambda
var_poisson = lambda
print('moyenne et variance de Poisson = ')  
print(lambda)

```
**3.**D'apres les resultats ci-dessus, la loi geometrique donne des resultats moins satisfaisants que la loi de Poisson. Le modele de Poisson est donc plus adapte. 

**4.** D'apres les resultats ci-dessous, on observe que la courbe de la densite geometrique se rapproche le plus de l'histogramme des valeurs empiriques. La premiere impression est donc infirmee. 
```{r}
#d<- density(discoveries)
#View(discoveries)
x = discoveries
plot(x, dgeom(discoveries, prob = theta, log=FALSE), main = 'densite geometrique', col='blue')
plot(x, dpois(discoveries, lambda, log = FALSE), main = 'densite de Poisson', col='red')
hist(discoveries, probability = TRUE)
#lines(d, col='black')

#plot(x,y1,type="l",col="red")
#lines(x,y2,col="green")
```



```{r}
library(ggplot2)
#x<-discoveries
#plot(x, qgeom(discoveries, theta), main='quantile inverse geom', col='blue')
#plot(x, qpois(discoveries, lambda), main='poisson', col='blue')
##attention aux invalid prob

#theta
#discoveries
#x<- sort(discoveries, decreasing = FALSE)
quantile_geom <- qgeom(discoveries, theta, lower.tail = TRUE, log.p = FALSE)

#plot(discoveries[i], quantile_geom[i/(n+1)], main = 'qq plot')


#1er quantile : i = 1
n <- length(discoveries)
summary(discoveries)

frst_quant <- summary(discoveries)[2]
thrd_quant <- summary(discoveries)[5]

x <- array(frst_quant, thrd_quant)
#y <- probleme 
plot(x, )

```
```{r}
##Utilisation de la fonction qqplot
#preparation des données
discoveries <- as.factor(discoveries)
head(discoveries)
a<-rgeom(n, theta)
b<-rpois(n, lambda)
qqplot(a, discoveries, main= 'qqplot loi géométrique et discoveries', col='blue')
qqline(a)
qqplot(b, discoveries, main= 'qqplot loi de Poisson et discoveries', col='red')
qqline(b)
```
**5.** La courbe de Henry de la distribution de Poisson convient le mieux. 
```{r}
c<-rchisq(n, 6, ncp=0.05)

qqplot(c, a)
qqline(a)
qqplot(c,b)
qqline(b)
#qqline(a, col='red')
#qqline(b, col='blue')
#qqline(c, col='grey')
```
**6.** Le modele qui convient le mieux est le modele de Poisson. 

## Exercice 2 : Analyse de l'incertitude dans le modele de Poisson
**1.** Les $(X_i)$ sont independantes et identiquement distribuees, suivant une loi de Poisson de parametre $\lambda$. Par addivite de la loi de Poisson, la variable aleatoire $(\sum{X_i})$ suit une loi de Poisson de parametre $n\lambda$. 
On pose $Y=\sum_{i=1}^{n}X_i$
Alors : $\forall s\ge1,  P(Y\ge s) = \sum_{k=s}^{+infty} P(Y=k) = \sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} \frac{(n \lambda)^k }{k!} $
En derivant terme a terme, on obtient : $\frac{\partial P(Y\ge s)}{\partial \lambda}=\sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} (-\frac{n^{k+1} \lambda^k}{k!} + \frac{n^k \lambda^{k-1}}{(k-1)!} )$
Il s'agit d'une serie entiere ayant pour terme general la difference de deux termes consecutifs. Ainsi, il vient :
$\frac{\partial P(Y\ge s)}{\partial \lambda}=\mathrm{e}^{-n\lambda} \frac{n^s \lambda^{s-1}}{(s-1)!} \ge 0$
Donc la fonction est croissante. 
