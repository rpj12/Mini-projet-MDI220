---
title: "Mini-projet MDI 220 Ruila Puskas-Juhasz"
output:
  html_notebook: default
  pdf_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
```{r}
data("discoveries")
plot(discoveries)
```
## Exercice 1
**1.** On cherche un estimateur $\hat{\theta}$ du jeu de donnees decrit par une loi geometrique. 
La densite de probabilite est donnee par $p(x, \theta) = \prod \theta (1-\theta)^{x_i}$ 
En prenant la log-vraisemblance on obtient $log(p(x,\theta)) = nlog(\theta) + \sum_{i=1}^{n} x_ilog(1-\theta)$
$\frac{\partial log(p(x,\theta))}{\partial \theta} = \frac{n}{\theta} - \frac{1}{1-\theta} \sum_{i=1}^{n} x_i = 0$
En annulant la derivee de cette expression par rapport à $\theta$, on obtient $\hat{\theta} = \frac{1}{1+\sum_{i=1}^{n} x_i}$

**2.** Dans le cas d'une loi de Poisson ou le parametre inconnu est $\lambda$, on trouve l'expression de $\hat{\lambda}$  en passant par la log-vraisemblance : 
$log(p(x, \lambda )) =  \sum_{} x_i log( \lambda ) -n\lambda - \sum_{} log(x_i !)$
En annulant la dérivée par rapport à $\lambda$ on obtient : 
$\frac{1}{\lambda} \sum{} x_i - n = 0$ 
$\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} x_i$
```{r}
#calcul de la moyenne et de la variance empirique du jeu de donnees
m = mean (discoveries)
v = var (discoveries)
print("moyenne empirique")
print(m)

print("variance empirique")
print(v)

#calcul à partir de l'estimateur de la loi géomtrique
n = length(discoveries)
S = 0 
for (i in (1:n)) {
  S <- S + discoveries[i]
}

theta = 1/((1/n)*S +1)
moy_geom = (1/n)*S + 1 ##1/theta
var_geom = (1-theta)/(theta**2)

print ('moyenne geometrique')
print(moy_geom)

print('variance geometrique')
print(var_geom)

#calcul à partir de l'estimateur de la loi de Poisson
lambda = (1/n)*S
moy_poisson = lambda
var_poisson = lambda
print('moyenne et variance de Poisson = ')  
print(lambda)

```
**3.**D'apres les resultats ci-dessus, la loi geometrique donne des resultats moins satisfaisants que la loi de Poisson. Le modele de Poisson est donc plus adapte. 
```{r}
print(m/moy_geom)
print(v/var_geom)

print(m/moy_poisson)
print(v/var_poisson)

print("les valeurs obtenues par la loi de Poisson sont plus proches des valeurs empiriques. La loi de Poisson approche le mieux la statistique consideree.")
```


**4.** D'apres les resultats ci-dessous, on observe que la courbe de la densite geometrique se rapproche le plus de l'histogramme des valeurs empiriques. La premiere impression est donc infirmee. 
Bleu : densite geometrique
Rouge : densite de Poisson
```{r}
x = 0:12
hist(discoveries, probability = TRUE, breaks = 12)
lines(x, dgeom(x, prob = theta, log=FALSE), main = 'densite geometrique', col='blue')
lines(x, dpois(x, lambda, log = FALSE), main = 'densite de Poisson', col='red')
```


**5.** La courbe de Henry de la distribution de Poisson convient le mieux. 
```{r}
#library(discovery)
y <- rpois(n, lambda)
## QQPlot pour la loi du Chi^2 comparée à la ditribution de Poisson:
qqplot(qchisq(ppoints(n), df = 6), y, main = expression("Q-Q plot for" ~~ {chi^2}[nu == 6]))

# Pour rajouter la ligne de modélisation 
library(ggplot2)
y = qpois(x, lambda)
p = 0.05
#ggplot2::last_plot() + qqline(y, distribution = function(p) qchisq(p, df = 6), prob = c(0.25, 0.75), col = 2)
##y doit être la loi de poisson

```

```{r}
library(ggplot2)
#x<-discoveries
#plot(x, qgeom(discoveries, theta), main='quantile inverse geom', col='blue')
#plot(x, qpois(discoveries, lambda), main='poisson', col='blue')
##attention aux invalid prob

#theta
#discoveries
#x<- sort(discoveries, decreasing = FALSE)
quantile_geom <- qgeom(discoveries, theta, lower.tail = TRUE, log.p = FALSE)

#plot(discoveries[i], quantile_geom[i/(n+1)], main = 'qq plot')


#1er quantile : i = 1
n <- length(discoveries)
summary(discoveries)

frst_quant <- summary(discoveries)[2]
thrd_quant <- summary(discoveries)[5]

x <- array(frst_quant, thrd_quant)
#y <- probleme 
#plot(x, )

```
```{r}
##Utilisation de la fonction qqplot
#preparation des données
discoveries <- as.factor(discoveries)
head(discoveries)
a<-rgeom(n, theta)
b<-rpois(n, lambda)
qqplot(a, discoveries, main= 'qqplot loi géométrique et discoveries', col='blue')
qqline(a)
qqplot(b, discoveries, main= 'qqplot loi de Poisson et discoveries', col='red')
qqline(b)
```
**6.** Le modele qui convient le mieux est le modele de Poisson. 
```{r}
c<-rchisq(n, 6, ncp=0.05)

qqplot(c, a)
#qqline(0.5*a-2.5, col='red')
qqplot(c,b)
#qqline(0.25*b-1.5, col='blue')
#qqline(a, col='red')
#qqline(b, col='blue')
#qqline(c, col='grey')
```


## Exercice 2 : Analyse de l'incertitude dans le modele de Poisson
**1.** Les $(X_i)$ sont independantes et identiquement distribuees, suivant une loi de Poisson de parametre $\lambda$. Par addivite de la loi de Poisson, la variable aleatoire $(\sum{X_i})$ suit une loi de Poisson de parametre $n\lambda$. 
On pose $Y=\sum_{i=1}^{n}X_i$
Alors : $\forall s\ge1,  P(Y\ge s) = \sum_{k=s}^{+infty} P(Y=k) = \sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} \frac{(n \lambda)^k }{k!} $
En derivant terme a terme, on obtient : $\frac{\partial P(Y\ge s)}{\partial \lambda}=\sum_{k=s}^{+infty} \mathrm{e}^{-n\lambda} (-\frac{n^{k+1} \lambda^k}{k!} + \frac{n^k \lambda^{k-1}}{(k-1)!} )$
Il s'agit d'une serie entiere ayant pour terme general la difference de deux termes consecutifs. Ainsi, il vient :
$\frac{\partial P(Y\ge s)}{\partial \lambda}=\mathrm{e}^{-n\lambda} \frac{n^s \lambda^{s-1}}{(s-1)!} \ge 0$
Donc la fonction est croissante. 

**2.** On en déduit un test de de l'hypothèse nulle. 
On a le test suivant : $H_0 : \lambda \le 3$ et $H_1 : \lambda > 3$ avec $\alpha \le 0.05$.
On rappelle que le modele est $\theta_0 = \lbrack 0,3 \rbrack$ et $\theta_1 = \rbrack 3, + \infty\lbrack$. Il s'agit de determiner un zone de rejet telle que, sous l'hypothese nulle, la probabilité que X appartienne à cette zone $X_1$ soit inférieure ou egale a $5/100$. On s'interesse à la statistique $\delta (T) = \mathbb{1} {T>s}$. 
On exprime le risque de premiere espece : $R(\theta_0 , \delta) = \alpha = P(\delta(T) = 1) \le 0.05$
$\Leftrightarrow \alpha = P(T > s) \Leftrightarrow \alpha = 1-P(T\le s) \Leftrightarrow P(T\le s) = 1- \alpha$
On retrouve l'expression de la fonction de répartition de T, dont la reciproque s'exprime comme le premier quantile $q_1$. D'ou : $s = q_1(1-\alpha)$. 
T suit une loi de Poisson de parametre $n\lambda$.
On utilise le rapport de vraisemblance monotone, pour cela verifions que la fonction de vraisemblance est strictement croisssante : $Z(x) = \frac{p_ \lambda_2 (x)} / {p_\lambda_1 (x)$  
```{r}
alpha = 0.05
print("s = ")
qpois(alpha, n*lambda)
```
**3.** En calculant la probabilité que T soit egal a 281 et en appliquant la croissance de la fonction, on obtient que le risque de premiere espece est nul. Donc on accepte $H_0$. 
```{r}
#calcul de la probabilite que T = 281
k <- 281
res <- lambda**(-k)*exp(-lambda)/factorial(k)
print(res)
```
```{r}
m <- mean(discoveries)

```

